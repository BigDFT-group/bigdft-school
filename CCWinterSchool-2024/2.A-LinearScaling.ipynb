{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-psQNxjEQqjP"
      },
      "source": [
        "# Linear Scaling BigDFT: Beyond Default Parameters\n",
        "\n",
        "In this notebook, we will explore some of the capabilites and behaviour of linear scaling BigDFT, going beyond the default parameters from the ```import: linear``` profile, using the example of glycine.\n",
        "\n",
        "This tutorial assumes that you are familiar with the basics of PyBigDFT and BigDFT, as well as the quick start linear scaling tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nPUVqXxZ2SLR"
      },
      "outputs": [],
      "source": [
        "install = \"client\" #@param [\"full_suite\", \"client\"]\n",
        "use_google_drive = False # @param {type:\"boolean\"}\n",
        "install_var=install\n",
        "!wget https://raw.githubusercontent.com/BigDFT-group/bigdft-school/data/packaging/install.py &> /dev/null\n",
        "args={'locally': True} if not use_google_drive else {}\n",
        "import install\n",
        "getattr(install,install_var.split()[0])(**args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.data_archive('data/ls_data.tar.gz')"
      ],
      "metadata": {
        "id": "M6jXsy9U2a_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijd0-yZPQqjU"
      },
      "source": [
        "## Support Function Parameters\n",
        "\n",
        "### Cubic vs Linear\n",
        "\n",
        "Previously, we saw how to run linear scaling (LS) BigDFT, now let's see how the results compare to cubic scaling (CS) BigDFT. For this tutorial, we will take the example of glycine. Let's read in and visualise the system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vg-Z7t9QqjV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from BigDFT.UnitCells import UnitCell\n",
        "from BigDFT.Visualization import InlineVisualizer\n",
        "from BigDFT.Systems import System\n",
        "from BigDFT.IO import read_xyz\n",
        "\n",
        "# first we read in the molecule\n",
        "name = 'glycine1'\n",
        "ifile = open(name+'.xyz')\n",
        "# we aren't going to use fragments for this tutorial, so we will read in the system as a single fragment\n",
        "gly1 = read_xyz(ifile, fragmentation='single')\n",
        "gly1.cell=UnitCell()\n",
        "ifile.close()\n",
        "\n",
        "# then we can visualise it (note the explicit usage of inlinevisualizer)\n",
        "viz = InlineVisualizer(300, 300)\n",
        "viz.display_system(gly1, zoom=1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8DWRzMQQqjX"
      },
      "source": [
        "Now let's set up our cubic and linear scaling input files, using the default linear profile as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOWruYjUQqjY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from BigDFT.Inputfiles import Inputfile\n",
        "\n",
        "inpc = Inputfile()\n",
        "inpl = Inputfile()\n",
        "\n",
        "for inp in [inpc, inpl]:\n",
        "    inp.set_hgrid(0.5)\n",
        "    inp.set_psp_nlcc()\n",
        "    inp.set_xc(\"PBE\")\n",
        "\n",
        "inpl[\"import\"] = \"linear\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihBRlEG9QqjY"
      },
      "source": [
        "And now let's setup the BigDFT calculator and run both calculations.\n",
        "\n",
        "**Pay attention**: In dry-run mode, the run timings are written in the existing logfiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5rCd3IjQqjZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from BigDFT.Calculators import SystemCalculator\n",
        "\n",
        "Ha2eV = 27.211396132\n",
        "\n",
        "calc = SystemCalculator(omp=2, mpi_run='mpirun -n 2', skip=True, verbose=False)\n",
        "\n",
        "run_cubic = calc.run(input=inpc, name=name+'_cubic', posinp=name+'.xyz')\n",
        "cubic_energy = Ha2eV * run_cubic.energy / run_cubic.nat\n",
        "cubic_time = run_cubic.log['Timings for root process']['Elapsed time (s)'] / 60.0\n",
        "\n",
        "print('Cubic scaling calculation took '+'{0:.1f}'.format(cubic_time)+' minutes, E = '+\\\n",
        "      '{0:.3f}'.format(cubic_energy)+' eV/atom')\n",
        "\n",
        "run_linear = calc.run(input=inpl, name=name+'_linear', posinp=name+'.xyz')\n",
        "linear_energy = Ha2eV * run_linear.energy / run_linear.nat\n",
        "linear_time = run_linear.log['Timings for root process']['Elapsed time (s)'] / 60.0\n",
        "\n",
        "print('Linear scaling calculation took '+'{0:.1f}'.format(linear_time)+' minutes, E = '+\\\n",
        "      '{0:.3f}'.format(linear_energy)+' eV/atom, E_linear - E_cubic = '+\\\n",
        "      '{0:.1f}'.format(1000.0 * (linear_energy - cubic_energy))+' meV/atom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4P9YswAQqja"
      },
      "source": [
        "Here we can notice two things:\n",
        "- The LS calculation takes much longer than the CS calculation. This is because the system is well below the crossover point.\n",
        "- The LS calculation has a higher energy than the CS calculation.\n",
        "\n",
        "However, the absolute energy difference is not very informative, so let's try running a different conformer of glycine, and see how the _relative_ energies compare.\n",
        "\n",
        "First let's visualise the two conformers side by side, so we can see the differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG5EKg9aQqjb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "name2 = 'glycine2'\n",
        "ifile = open(name2+'.xyz')\n",
        "gly2 = read_xyz(ifile, fragmentation='single')\n",
        "gly2.cell=UnitCell()\n",
        "ifile.close()\n",
        "\n",
        "molecules = []\n",
        "molecules.append(gly1)\n",
        "molecules.append(gly2)\n",
        "gridlist = [[0, 0], [0, 1]]\n",
        "colordict = [{\"FRA:0\": '#FF0000'}, {\"FRA:0\": '#0000FF'}]\n",
        "viz = InlineVisualizer(300, 300, nrow=1, ncol=2)\n",
        "viz.display_system(*molecules, gridlist=gridlist, colordict=colordict, zoom=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geMr94fOQqjc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "run_cubic2 = calc.run(input=inpc, name=name2+'_cubic', posinp=name2+'.xyz')\n",
        "deltae_cubic = Ha2eV * (run_cubic2.energy - run_cubic.energy)\n",
        "\n",
        "print('Cubic scaling Delta E = '+'{0:.3f}'.format(deltae_cubic)+' eV')\n",
        "\n",
        "run_linear2 = calc.run(input=inpl, name=name2+'_linear', posinp=name2+'.xyz')\n",
        "deltae_linear = Ha2eV * (run_linear2.energy - run_linear.energy)\n",
        "\n",
        "error = (deltae_linear - deltae_cubic) / run_cubic.nat\n",
        "print('Linear scaling Delta E = '+'{0:.3f}'.format(deltae_linear)+' eV, Delta E_linear - Delta E_cubic = '+\\\n",
        "      '{0:.1f}'.format(1000.0 * (error))+' meV/atom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQecilLbQqjc"
      },
      "source": [
        "As you can see, we benefit significantly from error cancellation, despite the fact that the support function optimisation means the basis set is not identical between the two conformers.\n",
        "\n",
        "### Convergence\n",
        "\n",
        "In many cases, the accuracy of the default linear profile is already sufficient. However, we may sometimes wish to push the accuracy further. In order to understand how, let's look at the input file in more detail.\n",
        "\n",
        "The support function (SF) basis is defined by a number of key parameters, which we can see if we inspect the ```lin_basis_params``` section of the logfile. Let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uulzJcJQqjd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# the basis params are defined on a per-element basis, with a default value for unlisted elements\n",
        "# let's just look at the elements which are in glycine\n",
        "elements = ['H', 'C', 'N', 'O']\n",
        "for element in elements:\n",
        "    print(element, run_linear.log['lin_basis_params'][element])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Xzek3EQqjd"
      },
      "source": [
        "As you can see, here we have a minimal basis, with only _s_-like SFs for H, and both _s_ and _p_-like SFs for C, N, O. You can also see that the localisation radii is smaller for H than for the other elements. If we want to increase the accuracy of our LS calculation, we can vary rloc.\n",
        "\n",
        "For simplicity, let's set all the elements to have the same localisation radius, and see how the total energy converges with repect to increasing localisation radius.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDL9XrT6Qqjd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define a range of sensible values -- too small and the results will be nonsensical\n",
        "rlocs = range(5, 10)\n",
        "\n",
        "rloc_runs = []\n",
        "for rloc in rlocs:\n",
        "    # define the input dictionary depending on the value of rloc\n",
        "    inpl.update({'lin_basis_params': {'H': {'nbasis': 1, 'rloc': rloc},\n",
        "                                     'C': {'nbasis': 4, 'rloc': rloc},\n",
        "                                     'N': {'nbasis': 4, 'rloc': rloc},\n",
        "                                     'O': {'nbasis': 4, 'rloc': rloc}}})\n",
        "\n",
        "    # run LS-BigDFT, saving the run to a list\n",
        "    rloc_runs.append(calc.run(input=inpl, name=name+'_rloc'+str(rloc), posinp=name+'.xyz'))\n",
        "\n",
        "    print('Calculation with rloc = '+str(rloc)+' is complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmjxxHdJQqje"
      },
      "source": [
        "Note that for larger localisation radii, it's also necessary to increase both ```rloc_kernel``` and ```rloc_kernel_foe```,  which define the localisation radii for the density kernel and the matrix vector muliplications used by the Fermi Operator Expansion approach. Both variables are also part of ```lin_basis_params```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY0CFd54Qqje"
      },
      "source": [
        "Let's plot the total energy convergence, using the cubic scaling energy as a reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5avtkqOQqje",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "\n",
        "ax.plot(rlocs, [1000.0 * Ha2eV * (run.energy - run_cubic.energy)/run.nat for run in rloc_runs],\n",
        "        color='k', marker='o')\n",
        "\n",
        "ax.set_ylabel('$E_{linear} - E_{cubic}$ (meV/atom)')\n",
        "ax.set_xlabel('$R_{\\mathrm{loc}}$ (bohr)')\n",
        "\n",
        "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKNt-i8yQqjf"
      },
      "source": [
        "As we can see, the error decreases monotonically with the localisation radius, being less than a few meV/atom for the largest radii.\n",
        "\n",
        "It's also interesting to see how the time varies. Let's generate a table summarising both the energies and timings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74s23zMUQqjf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "rows = ['LS, rloc='+str(rloc)+' bohr' for rloc in rlocs]\n",
        "rows += ['CS']\n",
        "columns = ['Total Energy (eV)', 'Error (meV/atom)', 'Total Time (min.)']\n",
        "\n",
        "table_data = []\n",
        "\n",
        "for run in rloc_runs:\n",
        "    table_data.append([Ha2eV * run.energy, 1000.0 * Ha2eV * (run.energy - run_cubic.energy) / run.nat,\n",
        "                 run.log['Timings for root process']['Elapsed time (s)'] / 60.0])\n",
        "\n",
        "table_data.append([Ha2eV * run_cubic.energy, 0.0, cubic_time])\n",
        "\n",
        "table = pd.DataFrame(table_data, index=rows, columns=columns)\n",
        "\n",
        "table.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PSqfULQqjf"
      },
      "source": [
        "Here we see that the walltime does not vary as systematically as the energy. This is because, while the cost of a given operation may increase due to larger localisation radii, the number of iterations may sometimes decrease when using larger radii.\n",
        "\n",
        "This behaviour is also very system dependent, since for this system size the matrices are fully dense, whereas for larger systems the sparsity pattern will influence the computational cost.\n",
        "\n",
        "### Exercise\n",
        "\n",
        "- Using the same input file, vary the localisation radius for the second glycine conformer.\n",
        "\n",
        "This excercise can also be performed in the skip mode, use the same naming scheme, i.e. the runs should be called 'glycine2_rloc{r}.yaml', where r is the localisation radius (this would ease the retrieval of dry-run data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UXnzu_2Qqjf"
      },
      "source": [
        "- Plot the convergence in the error of the _relative_ energies, again using the cubic scaling values as a reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgQRjuMpQqjg"
      },
      "source": [
        "You should find that the errors are much smaller than for the total energies, regardless of the localisation radius.\n",
        "\n",
        "You may also notice that the error does not tail off exactly to zero. This small error can be reduced by fine-tuning other parameters, such as the convergence thresholds.\n",
        "\n",
        "In additon, as well as varying the localisation radii, we can also increase the number of SFs associated with each species. This can be useful in the case of heavier elements, either to ensure better convergence or guided by the pseudopotential definition, as well as for excited state simulations, as discussed in the tutorial on transition-based constrained DFT. But for most systems containing only light elements, a minimal basis will be sufficient.\n",
        "\n",
        "It is also possible to change parameters relating to the SF opimisation, including the convergence threshold, number of SF optimisation iterations etc. These are defined in the ```lin_basis``` input block, and the default values are printed below. In most cases (with the exception of fragment calculations, as we will see in later tutorials), it will not be necessary to adapt any of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jmZPFpOQqjg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(run_linear.log['lin_basis'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X9zSguQQqji"
      },
      "source": [
        "## Density Kernel Parameters\n",
        "\n",
        "As well as the SF optimisation, we can also adjust the parameters relating to density kernel optimisation, which are defined in the ```lin_kernel``` block.\n",
        "\n",
        "Of these, one of the most important parameters is the approach used for kernel optimisation, which is set using the ```linear_method``` keyword.\n",
        "\n",
        "The linear scaling profile uses the Fermi Operator Expansion (```FOE```) approach, which is the recommended approach in most cases, particularly for very large systems. However, two other approaches are also available: direct minimisation of the Kohn Sham (KS) coefficients (```DIRMIN```), and diagonalisation (```DIAG```).\n",
        "\n",
        "These other approaches may be useful in different scenarios, as we will see.\n",
        "\n",
        "### Density of States\n",
        "\n",
        "First let's think about plotting a density of states (DoS). The LS runs we did all used FOE, which works directly with the density kernel, not the coefficients. As a result, in a standard calculation, we don't have access to the eigenvalues.\n",
        "\n",
        "If we want to plot the DoS we could use the diagonalisation approach instead, or we can still use FOE, and just tell BigDFT to do a single diagonalisation at the end. Let's do that and rerun one of the calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HribZaxfQqjj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# turn on subspace diagonalisation\n",
        "inpl.update({'lin_general': {'subspace_diag': True}})\n",
        "\n",
        "# reset to the default rloc\n",
        "inpl.pop('lin_basis_params')\n",
        "\n",
        "run_linear_dos = calc.run(input=inpl, name=name+'_linear_dos', posinp=name+'.xyz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShYfFpMwQqjj"
      },
      "source": [
        "We can now plot the DoS, comparing it to CS BigDFT (see the tutorial on the DoS for more information on how to extract and plot the DoS).\n",
        "\n",
        "Note that the Fermi level is not automatically retrieved for LS-BigDFT, so we'll use the CS Fermi level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb0KeD45Qqjj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from BigDFT.DoS import DoS\n",
        "\n",
        "dos = DoS(logfiles_dict={'Linear': run_linear_dos, 'Cubic': run_cubic},\n",
        "          fermi_level=run_cubic.fermi_level, units='eV')\n",
        "\n",
        "ax = dos.plot()\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8ZjY_4nQqjj"
      },
      "source": [
        "There are some small discrepancies, but overall the DoS for LS and CS are similar - as with the total energy, if better agreement is required, we could increase the localisation radii.\n",
        "\n",
        "Notice, however, that the virtual states are missing for the CS DoS. Let's rerun CS BigDFT, telling it to also calculate some empty states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seJTb78cQqjk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# calculate 10 virtual states using the Davidson approach after the SCF cycle\n",
        "# setting a subspace of 20 virtual orbitals\n",
        "inpc.extract_virtual_states(nvirt=10, davidson=True, norbv=20)\n",
        "\n",
        "run_cubic_dos = calc.run(input=inpc, name=name+'_cubic_dos', posinp=name+'.xyz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAp4TozUQqjk"
      },
      "source": [
        "Let's replot the DoS comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzCmay2SQqjk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dos = DoS(logfiles_dict={'Linear': run_linear_dos, 'Cubic': run_cubic_dos},\n",
        "          fermi_level=run_cubic.fermi_level, units='eV')\n",
        "\n",
        "ax = dos.plot()\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Qv1x1AQqjk"
      },
      "source": [
        "Now we have the virtual states for both approaches, we can see that the agreement above the Fermi level is very poor. This is expected, as the SFs have been optimised to represent the occupied states only. Also, the Cubic Scaling suffers of the so-called continuum collapse, where unoccupied states of positive energy (for molecular systems) tend to accumulate to very low energy values.\n",
        "\n",
        "It is possible to also optimise the SFs to represent the low-lying virtual states (with negative energy) using the direct minimisation approach, as is explained in more detail in the tutorial on transition-based constrained DFT.\n",
        "\n",
        "As with SF optimisation parameters, the rest of the density kernel optimisation parameters can be safely kept at their default values for most calculations, except in the case of fragment calculations, as we will also see in later tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLKOmrrbQqjk"
      },
      "source": [
        "## From Linear Scaling to Cubic Scaling\n",
        "\n",
        "Let's now briefly look at I/O in LS-BigDFT. By default, the profile will output the SF matrices (kernel, overlap etc.), however we may also wish to output the SFs themselves, either for visualisation, or for doing a restart calculation.\n",
        "\n",
        "This is activated by using the ```output_wf``` keyword in ```lin_general```. A restart calculation can then be performed starting from these SFs, which you can see an example of in the fragment tutorials.\n",
        "\n",
        "We also have another possibility - we can write the KS wavefunctions themselves, in a format which can be used for CS BigDFT. Let's try this now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXlzEuhAQqjl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# tell BigDFT to output the KS orbitals in text format\n",
        "inpl.update({'output': {'orbitals': 'text'}})\n",
        "\n",
        "run_linear_write = calc.run(input=inpl, name=name+'_linear_write', posinp=name+'.xyz')\n",
        "linear_write_energy = Ha2eV * run_linear_write.energy / run_linear_write.nat\n",
        "linear_write_time = run_linear_write.log['Timings for root process']['Elapsed time (s)']\n",
        "\n",
        "print('Linear scaling calculation took '+'{0:.1f}'.format(linear_write_time)+' seconds, E = '+\\\n",
        "      '{0:.4f}'.format(linear_write_energy)+' eV/atom')\n",
        "\n",
        "# and then run CS BigDFT using this as an input, without any further optimisation\n",
        "inpc['dft'].update({'inputpsiid': 2, 'itermax': 1})\n",
        "# we also need to tell CS BigDFT what directory the files are in\n",
        "inpc.update({'radical': name+'_linear_write'})\n",
        "# and let's turn off the virtual states\n",
        "inpc.extract_virtual_states(nvirt=0)\n",
        "\n",
        "run_cubic_read = calc.run(input=inpc, name=name+'_cubic_read', posinp=name+'.xyz')\n",
        "cubic_read_energy = Ha2eV * run_cubic_read.energy / run_cubic_read.nat\n",
        "cubic_read_time = run_cubic_read.log['Timings for root process']['Elapsed time (s)']\n",
        "\n",
        "print('Cubic scaling calculation took '+'{0:.1f}'.format(cubic_read_time)+' seconds, E = '+\\\n",
        "      '{0:.4f}'.format(cubic_read_energy)+' eV/atom')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcEExIdVQqjl"
      },
      "source": [
        "You can see that the two calculations have very similar energies, apart from a small numerical discrepancy, but since we did not further optimise the KS orbitals, the cubic scaling calculation was very quick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFhNeEnUQqjl"
      },
      "source": [
        "## Linear Scaling BigDFT and HPC\n",
        "\n",
        "In this notebook the calculations were performed only using a handful of cores, however for most interesting systems where LS-BigDFT is applicable, we will be using supercomputers, with very many cores. There are a few extra points to bear in mind for such calculations:\n",
        "- GPUs: Not available with LS-BigDFT.\n",
        "- OpenMP: Up to 8 threads can typically be used without too large a loss of efficiency, but for low memory nodes it may be necessary to use more threads to avoid under-occupying a node.\n",
        "- MPI: MPI tasks are primarily parellelised over SFs (also in places over KS orbitals as in CS-BigDFT), so there is a strict maximum upper limit of MPI tasks which can be used. For best performance, it is better to have at leat 5-10 SFs per MPI.\n",
        "\n",
        "We can check the parallelisation setup by inspecting the logfile:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni1CU4KKQqjm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print('MPI tasks = ',run_linear.log['Number of MPI tasks'])\n",
        "print('OpenMP threads = ',run_linear.log['Maximal OpenMP threads per MPI task'])\n",
        "\n",
        "print('Orbitals repartition', run_linear.log['Orbitals Repartition'])\n",
        "print('Support function repartition',run_linear.log['Support Function Repartition'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KktjrstQqjm"
      },
      "source": [
        "Here we can easily see that we have enough SFs per MPI to run efficiently, with each task having enough work and a well-balanced setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Il9CG4DQqjm"
      },
      "source": [
        "## When to use Linear Scaling BigDFT\n",
        "\n",
        "As a final note, we may ask how to decide when to use LS vs CS BigDFT. This may depend on both the functionality we wish to use, e.g. hybrid functionals and _k_-points are only available in CS-BigDFT, whereas the complexity reduction framework is only compatible with LS-BigDFT.\n",
        "\n",
        "In addition, the applicability of each method will depend on the size of the system. For small systems, CS will be cheaper due to the smaller prefactor, as we already saw in this tutorial. The crossover point at which LS becomes cheaper is system dependent and influenced by a range of factors, including band gap, dimensionality, and various input parameters such as the localisation radii. Therefore it may be important to investigate on a system-dependent basis, but as a rule of thumb, if the main criterion is performance, above 1000 atoms it will usually be cheaper to use LS-BigDFT. When the investigation is motivated by the need of having precise results (for a given choice of the model, PSP, XC, etc.) the CS approach is the preferred choice if the system has a compatible size. For systems of any size, though, when the main motivation is the investigation of localized quantities, the LS-BigDFT is the method of choice, for a compatible functionality of course."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}